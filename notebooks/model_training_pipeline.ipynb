{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing modularized functions\n",
    "df_path = \"\" # edit this to select dataset\n",
    "df = pd.read_csv(df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(df: pd.DataFrame):\n",
    "    '''\n",
    "    Removes transcript_name and gene_id columns.\n",
    "    Add or remove columns as required.\n",
    "    '''\n",
    "    df1 = df.drop([\"transcript_name\", \"gene_id\", \"nucleotide_seq\"], axis=1)\n",
    "    return df1\n",
    "\n",
    "def create_vectorizer(df:pd.DataFrame, n):\n",
    "    '''\n",
    "    to vectorize nucleotide sequences using corpus from training dataset\n",
    "    df should be training dataset, n would be value for ngrams\n",
    "    '''\n",
    "    corpus = df['nucleotide_seq']\n",
    "    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(n,n)).fit(corpus)\n",
    "    return vectorizer\n",
    "\n",
    "def trigram_tokenize(df:pd.DataFrame, vectorizer):\n",
    "    '''\n",
    "    to apply trigram on column \"nucleotide_seq\" using TfidfVectorizer\n",
    "    '''\n",
    "    v_nucleotide_seq = vectorizer.transform(df[\"nucleotide_seq\"])\n",
    "    v = pd.DataFrame(v_nucleotide_seq.toarray()) # convert sparse matrix to array\n",
    "\n",
    "    # creating dictionary to easily add vectorized sequence features as columns to dataframe\n",
    "    new_nucleotide_data = dict() \n",
    "    for i in range(v.shape[1]):\n",
    "        key = \"s\" + str(i)\n",
    "        new_nucleotide_data[key] = v.iloc[:,i]\n",
    "\n",
    "    df_final = df.assign(**new_nucleotide_data)\n",
    "    df_final = df_final.fillna(0)\n",
    "    \n",
    "    return df_final #returns dataframe with vectorized nucleotide features as columns\n",
    "\n",
    "def data_split(df:pd.DataFrame):\n",
    "    '''\n",
    "    splits data by gene into train and test sets according to the percentage given.\n",
    "    '''\n",
    "    X = df.drop([\"label\"], axis=1)\n",
    "    y = df[\"label\"]\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=2, random_state=0, test_size=0.2)\n",
    "    train_i, test_i = next(gss.split(X,y,groups=X.gene_id))\n",
    "\n",
    "    X_train = X.loc[train_i]\n",
    "    y_train = y.loc[train_i]\n",
    "\n",
    "    X_test = X.loc[test_i]\n",
    "    y_test = y.loc[test_i]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def create_standardizer(df:pd.DataFrame):\n",
    "    '''\n",
    "    create standardizer based on training dataset to standardize other datasets\n",
    "    df should be X_train\n",
    "    '''\n",
    "    # Identify numerical and categorical columns\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = df.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "    standardizer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_cols),\n",
    "            ('cat', 'passthrough', categorical_cols)  # Leave categorical columns unchanged\n",
    "        ])\n",
    "    \n",
    "    standardizer.fit(df)\n",
    "\n",
    "    return standardizer\n",
    "\n",
    "def standardize_data(df:pd.DataFrame, standardizer):\n",
    "    '''\n",
    "    Standardizes numerical features while leaving non-numerical features unchanged using fitted preprocesser from create_standardizer function\n",
    "    '''\n",
    "    x_columns = df.columns\n",
    "    df_scaled = standardizer.transform(df)\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns = x_columns)\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "def synthetic_oversampling(X_train, y_train):\n",
    "    '''\n",
    "    Uses SMOTE to oversample the minority class.\n",
    "    '''\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    return X_train_resampled, y_train_resampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stringing together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_all(df):\n",
    "    \"\"\"\n",
    "    Purpose: Setting up the pipeline? to drop unnecessary columns, followed by vectorizing, splitting, standardising and oversampling\n",
    "    \"\"\"\n",
    "    # Train-test split and remove the \"transcript_name\", \"gene_id\", \"nucleotide_seq\" columns\n",
    "    Xtrain, Xtest, ytrain, ytest = data_split(df)\n",
    "\n",
    "    # Vectorization\n",
    "    vectorizer = create_vectorizer(Xtrain,3)\n",
    "    Xtrain_v = trigram_tokenize(Xtrain, vectorizer)\n",
    "    Xtest_v = trigram_tokenize(Xtest,vectorizer)\n",
    "\n",
    "    # Remove the \"transcript_name\", \"gene_id\", \"nucleotide_seq\" columns\n",
    "    Xtrain_v = remove_columns(Xtrain_v)\n",
    "    Xtest_v = remove_columns(Xtest_v)\n",
    "\n",
    "    # Scaling Xtrain_v and Xtest_v with Scaler fitted on Xtrain\n",
    "    standardizer = create_standardizer(Xtrain_v)\n",
    "    Xtrain_scaled = standardize_data(Xtrain_v, standardizer)\n",
    "    Xtest_scaled = standardize_data(Xtest_v, standardizer)\n",
    "\n",
    "    # SMOTE on training set \n",
    "    Xtrain_resampled, ytrain_resampled = synthetic_oversampling(Xtrain_scaled, ytrain)\n",
    "    \n",
    "    return Xtrain_resampled, Xtest_scaled, ytrain_resampled, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = pipeline_all(df)\n",
    "X_train.to_csv(\"../../dataset/Xtrain.csv\", index = False)\n",
    "y_train.to_csv(\"../../dataset/ytrain.csv\", index = False)\n",
    "X_test.to_csv(\"../../dataset/Xtest.csv\", index = False)\n",
    "y_test.to_csv(\"../../dataset/ytest.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
