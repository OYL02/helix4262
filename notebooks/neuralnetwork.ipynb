{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network Model\n",
    "\n",
    "- using `pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the 'code' directory\n",
    "sys.path.append(os.path.abspath('../code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# include feature engineering pipeline\n",
    "from feature_eng_pipeline import pipeline_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# switch to using CUDA - GPU\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "data_path = '../../data/mean_with_labels.csv'\n",
    "\n",
    "class RNANanoporeDataset(Dataset):\n",
    "    \"\"\"Dataset used to train and test RNA Nanopore data\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"Initializes instance of class RNANanoporeDataset.\n",
    "\n",
    "        Args:\n",
    "            csv_file (str): Path to the csv file with the nanopore data\n",
    "        \"\"\"\n",
    "\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        v, s, X_df, y_df = pipeline_nn(self.df)\n",
    "        X_drop = X_df.drop([\"transcript_name\", \"gene_id\", \"nucleotide_seq\"], axis=1).reset_index(drop=True)  \n",
    "\n",
    "        # TODO: for now we drop all trigram columns\n",
    "        self.X = X_drop[[\"json_position\", \"dwelling_time_min1\", \"sd_min1\", \"mean_min1\", \"dwelling_time\", \"sd\", \"mean\", \"dwelling_time_plus1\", \"sd_plus1\", \"mean_plus1\"]]\n",
    "        self.y = y_df.reset_index(drop=True).squeeze()  \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the size of the dataset\"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    # Handle if idx is a tensor (converting to list if needed)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        signal_features = self.X.iloc[idx].values  \n",
    "        label = self.y.iloc[idx]  \n",
    "\n",
    "        # Convert to tensors\n",
    "        signal_features = torch.tensor(signal_features, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return signal_features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final datasetp preview:    json_position  dwelling_time_min1   sd_min1  mean_min1  dwelling_time  \\\n",
      "0      -1.167740            0.205115 -0.093805  -1.129683       0.162595   \n",
      "1      -1.167740           -1.243149 -0.135977  -1.120311      -0.879851   \n",
      "2      -1.166286           -0.834348  2.163181   0.889406       1.298831   \n",
      "3      -1.166286           -0.068915 -0.988879  -0.556731      -0.627451   \n",
      "4      -1.165559            0.602928 -0.207387   0.883266      -0.869697   \n",
      "\n",
      "         sd      mean  dwelling_time_plus1  sd_plus1  mean_plus1  \n",
      "0  1.967826  0.531283            -0.868368  0.233028   -1.451873  \n",
      "1  1.899411  0.572909            -0.352487  0.843539   -2.066144  \n",
      "2 -0.595935 -1.297428            -0.779589 -1.095111    0.038191  \n",
      "3 -0.497492 -0.996726            -1.390108  0.406858   -1.292909  \n",
      "4  0.594147  0.764529             0.004848  0.661955   -0.154392  \n"
     ]
    }
   ],
   "source": [
    "# preparing data for training using DataLoaders\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "dataset = RNANanoporeDataset(data_path)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "trainset, testset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Dataloaders\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModNet(nn.Module):\n",
    "    def __init__(self, signal_input_dim):\n",
    "        super(ModNet, self).__init__()\n",
    "\n",
    "        # Read-level Encoder: MLP with two hidden layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(signal_input_dim, 150),  # Change hidden dimensions as needed\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(150, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # Single output for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, signal_features):\n",
    "        read_level_probs = self.encoder(signal_features)\n",
    "        return torch.sigmoid(read_level_probs)  # Apply sigmoid for probabilities\n",
    "\n",
    "\n",
    "    def noisy_or_pooling(self, read_level_probs):\n",
    "        \"\"\"\n",
    "        :param read_level_probs: Tensor of shape (batch_size, 1)\n",
    "        :return: Site-level modification probability for each site (batch_size, 1)\n",
    "        \"\"\"\n",
    "        site_level_probs = 1 - torch.prod(1 - read_level_probs, dim=1)\n",
    "        return site_level_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA launch blocking for better error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that ModNet is already defined\n",
    "model = ModNet(signal_input_dim=10) \n",
    "               #trigram_vocab_size=64,\n",
    "               #embedding_dim=20)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [10], Loss: 0.7113\n",
      "Epoch [1/10], Batch [20], Loss: 0.6950\n",
      "Epoch [1/10], Batch [30], Loss: 0.6853\n",
      "Epoch [1/10], Batch [40], Loss: 0.6837\n",
      "Epoch [1/10], Batch [50], Loss: 0.6781\n",
      "Epoch [1/10], Batch [60], Loss: 0.6720\n",
      "Epoch [1/10], Batch [70], Loss: 0.6659\n",
      "Epoch [1/10], Batch [80], Loss: 0.6644\n",
      "Epoch [1/10], Batch [90], Loss: 0.6586\n",
      "Epoch [1/10], Batch [100], Loss: 0.6549\n",
      "Epoch [1/10], Batch [110], Loss: 0.6553\n",
      "Epoch [1/10], Batch [120], Loss: 0.6466\n",
      "Epoch [1/10], Batch [130], Loss: 0.6384\n",
      "Epoch [1/10], Batch [140], Loss: 0.6357\n",
      "Epoch [1/10], Batch [150], Loss: 0.6356\n",
      "Epoch [1/10], Batch [160], Loss: 0.6321\n",
      "Epoch [1/10], Batch [170], Loss: 0.6276\n",
      "Epoch [1/10], Batch [180], Loss: 0.6225\n",
      "Epoch [1/10], Batch [190], Loss: 0.6255\n",
      "Epoch [1/10], Batch [200], Loss: 0.6209\n",
      "Epoch [1/10], Batch [210], Loss: 0.6272\n",
      "Epoch [1/10], Batch [220], Loss: 0.6141\n",
      "Epoch [1/10], Batch [230], Loss: 0.6128\n",
      "Epoch [1/10], Batch [240], Loss: 0.6200\n",
      "Epoch [1/10], Batch [250], Loss: 0.6136\n",
      "Epoch [1/10], Batch [260], Loss: 0.6148\n",
      "Epoch [1/10], Batch [270], Loss: 0.6148\n",
      "Epoch [1/10], Batch [280], Loss: 0.6043\n",
      "Epoch [1/10], Batch [290], Loss: 0.6110\n",
      "Epoch [1/10], Batch [300], Loss: 0.6148\n",
      "Epoch [1/10], Batch [310], Loss: 0.6084\n",
      "Epoch [1/10], Batch [320], Loss: 0.6082\n",
      "Epoch [1/10], Batch [330], Loss: 0.6038\n",
      "Epoch [1/10], Batch [340], Loss: 0.6108\n",
      "Epoch [1/10], Batch [350], Loss: 0.6100\n",
      "Epoch [1/10], Batch [360], Loss: 0.6058\n",
      "Epoch [1/10], Batch [370], Loss: 0.6039\n",
      "Epoch [1/10], Batch [380], Loss: 0.6025\n",
      "Epoch [1/10], Batch [390], Loss: 0.6007\n",
      "Epoch [1/10], Batch [400], Loss: 0.6022\n",
      "Epoch [1/10], Batch [410], Loss: 0.6066\n",
      "Epoch [1/10], Batch [420], Loss: 0.6078\n",
      "Epoch [1/10], Batch [430], Loss: 0.6098\n",
      "Epoch [1/10], Batch [440], Loss: 0.5994\n",
      "Epoch [1/10], Batch [450], Loss: 0.5964\n",
      "Epoch [1/10], Batch [460], Loss: 0.5993\n",
      "Epoch [1/10], Batch [470], Loss: 0.6053\n",
      "Epoch [1/10], Batch [480], Loss: 0.5988\n",
      "Epoch [1/10], Batch [490], Loss: 0.5911\n",
      "Epoch [1/10], Batch [500], Loss: 0.5940\n",
      "Epoch [1/10], Batch [510], Loss: 0.5961\n",
      "Epoch [1/10], Batch [520], Loss: 0.5992\n",
      "Epoch [1/10], Batch [530], Loss: 0.5976\n",
      "Epoch [1/10], Batch [540], Loss: 0.5974\n",
      "Epoch [1/10], Batch [550], Loss: 0.5877\n",
      "Epoch [1/10], Batch [560], Loss: 0.5976\n",
      "Epoch [1/10], Batch [570], Loss: 0.5955\n",
      "Epoch [1/10], Batch [580], Loss: 0.5972\n",
      "Epoch [1/10], Batch [590], Loss: 0.5913\n",
      "Epoch [1/10], Batch [600], Loss: 0.5947\n",
      "Epoch [1/10], Batch [610], Loss: 0.5906\n",
      "Epoch [1/10], Batch [620], Loss: 0.5960\n",
      "Epoch [1/10], Batch [630], Loss: 0.6011\n",
      "Epoch [1/10], Batch [640], Loss: 0.5995\n",
      "Epoch [1/10], Batch [650], Loss: 0.5897\n",
      "Epoch [2/10], Batch [10], Loss: 0.5964\n",
      "Epoch [2/10], Batch [20], Loss: 0.5975\n",
      "Epoch [2/10], Batch [30], Loss: 0.5935\n",
      "Epoch [2/10], Batch [40], Loss: 0.5932\n",
      "Epoch [2/10], Batch [50], Loss: 0.5908\n",
      "Epoch [2/10], Batch [60], Loss: 0.5914\n",
      "Epoch [2/10], Batch [70], Loss: 0.5946\n",
      "Epoch [2/10], Batch [80], Loss: 0.5825\n",
      "Epoch [2/10], Batch [90], Loss: 0.5882\n",
      "Epoch [2/10], Batch [100], Loss: 0.5925\n",
      "Epoch [2/10], Batch [110], Loss: 0.5913\n",
      "Epoch [2/10], Batch [120], Loss: 0.5885\n",
      "Epoch [2/10], Batch [130], Loss: 0.5920\n",
      "Epoch [2/10], Batch [140], Loss: 0.5884\n",
      "Epoch [2/10], Batch [150], Loss: 0.5920\n",
      "Epoch [2/10], Batch [160], Loss: 0.5941\n",
      "Epoch [2/10], Batch [170], Loss: 0.5876\n",
      "Epoch [2/10], Batch [180], Loss: 0.5862\n",
      "Epoch [2/10], Batch [190], Loss: 0.5936\n",
      "Epoch [2/10], Batch [200], Loss: 0.5910\n",
      "Epoch [2/10], Batch [210], Loss: 0.5832\n",
      "Epoch [2/10], Batch [220], Loss: 0.5919\n",
      "Epoch [2/10], Batch [230], Loss: 0.5841\n",
      "Epoch [2/10], Batch [240], Loss: 0.5929\n",
      "Epoch [2/10], Batch [250], Loss: 0.5903\n",
      "Epoch [2/10], Batch [260], Loss: 0.5947\n",
      "Epoch [2/10], Batch [270], Loss: 0.5879\n",
      "Epoch [2/10], Batch [280], Loss: 0.5885\n",
      "Epoch [2/10], Batch [290], Loss: 0.5930\n",
      "Epoch [2/10], Batch [300], Loss: 0.5890\n",
      "Epoch [2/10], Batch [310], Loss: 0.5918\n",
      "Epoch [2/10], Batch [320], Loss: 0.5925\n",
      "Epoch [2/10], Batch [330], Loss: 0.5870\n",
      "Epoch [2/10], Batch [340], Loss: 0.5955\n",
      "Epoch [2/10], Batch [350], Loss: 0.5908\n",
      "Epoch [2/10], Batch [360], Loss: 0.5848\n",
      "Epoch [2/10], Batch [370], Loss: 0.5950\n",
      "Epoch [2/10], Batch [380], Loss: 0.5917\n",
      "Epoch [2/10], Batch [390], Loss: 0.5831\n",
      "Epoch [2/10], Batch [400], Loss: 0.5879\n",
      "Epoch [2/10], Batch [410], Loss: 0.5918\n",
      "Epoch [2/10], Batch [420], Loss: 0.5866\n",
      "Epoch [2/10], Batch [430], Loss: 0.5856\n",
      "Epoch [2/10], Batch [440], Loss: 0.5918\n",
      "Epoch [2/10], Batch [450], Loss: 0.5882\n",
      "Epoch [2/10], Batch [460], Loss: 0.5917\n",
      "Epoch [2/10], Batch [470], Loss: 0.5799\n",
      "Epoch [2/10], Batch [480], Loss: 0.5936\n",
      "Epoch [2/10], Batch [490], Loss: 0.5883\n",
      "Epoch [2/10], Batch [500], Loss: 0.5849\n",
      "Epoch [2/10], Batch [510], Loss: 0.5849\n",
      "Epoch [2/10], Batch [520], Loss: 0.5891\n",
      "Epoch [2/10], Batch [530], Loss: 0.5916\n",
      "Epoch [2/10], Batch [540], Loss: 0.5982\n",
      "Epoch [2/10], Batch [550], Loss: 0.5853\n",
      "Epoch [2/10], Batch [560], Loss: 0.5910\n",
      "Epoch [2/10], Batch [570], Loss: 0.5908\n",
      "Epoch [2/10], Batch [580], Loss: 0.5919\n",
      "Epoch [2/10], Batch [590], Loss: 0.5869\n",
      "Epoch [2/10], Batch [600], Loss: 0.5860\n",
      "Epoch [2/10], Batch [610], Loss: 0.5811\n",
      "Epoch [2/10], Batch [620], Loss: 0.5903\n",
      "Epoch [2/10], Batch [630], Loss: 0.5872\n",
      "Epoch [2/10], Batch [640], Loss: 0.5892\n",
      "Epoch [2/10], Batch [650], Loss: 0.5890\n",
      "Epoch [3/10], Batch [10], Loss: 0.5884\n",
      "Epoch [3/10], Batch [20], Loss: 0.5882\n",
      "Epoch [3/10], Batch [30], Loss: 0.5893\n",
      "Epoch [3/10], Batch [40], Loss: 0.5880\n",
      "Epoch [3/10], Batch [50], Loss: 0.5871\n",
      "Epoch [3/10], Batch [60], Loss: 0.5863\n",
      "Epoch [3/10], Batch [70], Loss: 0.5925\n",
      "Epoch [3/10], Batch [80], Loss: 0.5836\n",
      "Epoch [3/10], Batch [90], Loss: 0.5783\n",
      "Epoch [3/10], Batch [100], Loss: 0.5842\n",
      "Epoch [3/10], Batch [110], Loss: 0.5903\n",
      "Epoch [3/10], Batch [120], Loss: 0.5895\n",
      "Epoch [3/10], Batch [130], Loss: 0.5875\n",
      "Epoch [3/10], Batch [140], Loss: 0.5827\n",
      "Epoch [3/10], Batch [150], Loss: 0.5947\n",
      "Epoch [3/10], Batch [160], Loss: 0.5883\n",
      "Epoch [3/10], Batch [170], Loss: 0.5830\n",
      "Epoch [3/10], Batch [180], Loss: 0.5831\n",
      "Epoch [3/10], Batch [190], Loss: 0.5851\n",
      "Epoch [3/10], Batch [200], Loss: 0.5947\n",
      "Epoch [3/10], Batch [210], Loss: 0.5861\n",
      "Epoch [3/10], Batch [220], Loss: 0.5872\n",
      "Epoch [3/10], Batch [230], Loss: 0.5847\n",
      "Epoch [3/10], Batch [240], Loss: 0.5870\n",
      "Epoch [3/10], Batch [250], Loss: 0.5867\n",
      "Epoch [3/10], Batch [260], Loss: 0.5836\n",
      "Epoch [3/10], Batch [270], Loss: 0.5919\n",
      "Epoch [3/10], Batch [280], Loss: 0.5878\n",
      "Epoch [3/10], Batch [290], Loss: 0.5888\n",
      "Epoch [3/10], Batch [300], Loss: 0.5798\n",
      "Epoch [3/10], Batch [310], Loss: 0.5869\n",
      "Epoch [3/10], Batch [320], Loss: 0.5897\n",
      "Epoch [3/10], Batch [330], Loss: 0.5776\n",
      "Epoch [3/10], Batch [340], Loss: 0.5853\n",
      "Epoch [3/10], Batch [350], Loss: 0.5815\n",
      "Epoch [3/10], Batch [360], Loss: 0.5859\n",
      "Epoch [3/10], Batch [370], Loss: 0.5894\n",
      "Epoch [3/10], Batch [380], Loss: 0.5882\n",
      "Epoch [3/10], Batch [390], Loss: 0.5888\n",
      "Epoch [3/10], Batch [400], Loss: 0.5858\n",
      "Epoch [3/10], Batch [410], Loss: 0.5862\n",
      "Epoch [3/10], Batch [420], Loss: 0.5806\n",
      "Epoch [3/10], Batch [430], Loss: 0.5874\n",
      "Epoch [3/10], Batch [440], Loss: 0.5865\n",
      "Epoch [3/10], Batch [450], Loss: 0.5907\n",
      "Epoch [3/10], Batch [460], Loss: 0.5841\n",
      "Epoch [3/10], Batch [470], Loss: 0.5898\n",
      "Epoch [3/10], Batch [480], Loss: 0.5917\n",
      "Epoch [3/10], Batch [490], Loss: 0.5770\n",
      "Epoch [3/10], Batch [500], Loss: 0.5815\n",
      "Epoch [3/10], Batch [510], Loss: 0.5875\n",
      "Epoch [3/10], Batch [520], Loss: 0.5851\n",
      "Epoch [3/10], Batch [530], Loss: 0.5829\n",
      "Epoch [3/10], Batch [540], Loss: 0.5814\n",
      "Epoch [3/10], Batch [550], Loss: 0.5850\n",
      "Epoch [3/10], Batch [560], Loss: 0.5892\n",
      "Epoch [3/10], Batch [570], Loss: 0.5815\n",
      "Epoch [3/10], Batch [580], Loss: 0.5909\n",
      "Epoch [3/10], Batch [590], Loss: 0.5891\n",
      "Epoch [3/10], Batch [600], Loss: 0.5876\n",
      "Epoch [3/10], Batch [610], Loss: 0.5861\n",
      "Epoch [3/10], Batch [620], Loss: 0.5811\n",
      "Epoch [3/10], Batch [630], Loss: 0.5850\n",
      "Epoch [3/10], Batch [640], Loss: 0.5814\n",
      "Epoch [3/10], Batch [650], Loss: 0.5855\n",
      "Epoch [4/10], Batch [10], Loss: 0.5821\n",
      "Epoch [4/10], Batch [20], Loss: 0.5860\n",
      "Epoch [4/10], Batch [30], Loss: 0.5802\n",
      "Epoch [4/10], Batch [40], Loss: 0.5810\n",
      "Epoch [4/10], Batch [50], Loss: 0.5795\n",
      "Epoch [4/10], Batch [60], Loss: 0.5865\n",
      "Epoch [4/10], Batch [70], Loss: 0.5846\n",
      "Epoch [4/10], Batch [80], Loss: 0.5882\n",
      "Epoch [4/10], Batch [90], Loss: 0.5764\n",
      "Epoch [4/10], Batch [100], Loss: 0.5856\n",
      "Epoch [4/10], Batch [110], Loss: 0.5837\n",
      "Epoch [4/10], Batch [120], Loss: 0.5876\n",
      "Epoch [4/10], Batch [130], Loss: 0.5753\n",
      "Epoch [4/10], Batch [140], Loss: 0.5853\n",
      "Epoch [4/10], Batch [150], Loss: 0.5860\n",
      "Epoch [4/10], Batch [160], Loss: 0.5828\n",
      "Epoch [4/10], Batch [170], Loss: 0.5864\n",
      "Epoch [4/10], Batch [180], Loss: 0.5839\n",
      "Epoch [4/10], Batch [190], Loss: 0.5820\n",
      "Epoch [4/10], Batch [200], Loss: 0.5873\n",
      "Epoch [4/10], Batch [210], Loss: 0.5920\n",
      "Epoch [4/10], Batch [220], Loss: 0.5841\n",
      "Epoch [4/10], Batch [230], Loss: 0.5893\n",
      "Epoch [4/10], Batch [240], Loss: 0.5828\n",
      "Epoch [4/10], Batch [250], Loss: 0.5864\n",
      "Epoch [4/10], Batch [260], Loss: 0.5888\n",
      "Epoch [4/10], Batch [270], Loss: 0.5874\n",
      "Epoch [4/10], Batch [280], Loss: 0.5847\n",
      "Epoch [4/10], Batch [290], Loss: 0.5810\n",
      "Epoch [4/10], Batch [300], Loss: 0.5836\n",
      "Epoch [4/10], Batch [310], Loss: 0.5881\n",
      "Epoch [4/10], Batch [320], Loss: 0.5830\n",
      "Epoch [4/10], Batch [330], Loss: 0.5758\n",
      "Epoch [4/10], Batch [340], Loss: 0.5870\n",
      "Epoch [4/10], Batch [350], Loss: 0.5807\n",
      "Epoch [4/10], Batch [360], Loss: 0.5800\n",
      "Epoch [4/10], Batch [370], Loss: 0.5850\n",
      "Epoch [4/10], Batch [380], Loss: 0.5912\n",
      "Epoch [4/10], Batch [390], Loss: 0.5868\n",
      "Epoch [4/10], Batch [400], Loss: 0.5831\n",
      "Epoch [4/10], Batch [410], Loss: 0.5869\n",
      "Epoch [4/10], Batch [420], Loss: 0.5828\n",
      "Epoch [4/10], Batch [430], Loss: 0.5787\n",
      "Epoch [4/10], Batch [440], Loss: 0.5821\n",
      "Epoch [4/10], Batch [450], Loss: 0.5881\n",
      "Epoch [4/10], Batch [460], Loss: 0.5822\n",
      "Epoch [4/10], Batch [470], Loss: 0.5924\n",
      "Epoch [4/10], Batch [480], Loss: 0.5878\n",
      "Epoch [4/10], Batch [490], Loss: 0.5822\n",
      "Epoch [4/10], Batch [500], Loss: 0.5850\n",
      "Epoch [4/10], Batch [510], Loss: 0.5867\n",
      "Epoch [4/10], Batch [520], Loss: 0.5866\n",
      "Epoch [4/10], Batch [530], Loss: 0.5820\n",
      "Epoch [4/10], Batch [540], Loss: 0.5858\n",
      "Epoch [4/10], Batch [550], Loss: 0.5846\n",
      "Epoch [4/10], Batch [560], Loss: 0.5847\n",
      "Epoch [4/10], Batch [570], Loss: 0.5830\n",
      "Epoch [4/10], Batch [580], Loss: 0.5844\n",
      "Epoch [4/10], Batch [590], Loss: 0.5781\n",
      "Epoch [4/10], Batch [600], Loss: 0.5858\n",
      "Epoch [4/10], Batch [610], Loss: 0.5817\n",
      "Epoch [4/10], Batch [620], Loss: 0.5808\n",
      "Epoch [4/10], Batch [630], Loss: 0.5855\n",
      "Epoch [4/10], Batch [640], Loss: 0.5792\n",
      "Epoch [4/10], Batch [650], Loss: 0.5868\n",
      "Epoch [5/10], Batch [10], Loss: 0.5889\n",
      "Epoch [5/10], Batch [20], Loss: 0.5794\n",
      "Epoch [5/10], Batch [30], Loss: 0.5842\n",
      "Epoch [5/10], Batch [40], Loss: 0.5784\n",
      "Epoch [5/10], Batch [50], Loss: 0.5886\n",
      "Epoch [5/10], Batch [60], Loss: 0.5838\n",
      "Epoch [5/10], Batch [70], Loss: 0.5750\n",
      "Epoch [5/10], Batch [80], Loss: 0.5755\n",
      "Epoch [5/10], Batch [90], Loss: 0.5899\n",
      "Epoch [5/10], Batch [100], Loss: 0.5822\n",
      "Epoch [5/10], Batch [110], Loss: 0.5768\n",
      "Epoch [5/10], Batch [120], Loss: 0.5832\n",
      "Epoch [5/10], Batch [130], Loss: 0.5814\n",
      "Epoch [5/10], Batch [140], Loss: 0.5773\n",
      "Epoch [5/10], Batch [150], Loss: 0.5727\n",
      "Epoch [5/10], Batch [160], Loss: 0.5858\n",
      "Epoch [5/10], Batch [170], Loss: 0.5847\n",
      "Epoch [5/10], Batch [180], Loss: 0.5863\n",
      "Epoch [5/10], Batch [190], Loss: 0.5885\n",
      "Epoch [5/10], Batch [200], Loss: 0.5803\n",
      "Epoch [5/10], Batch [210], Loss: 0.5785\n",
      "Epoch [5/10], Batch [220], Loss: 0.5862\n",
      "Epoch [5/10], Batch [230], Loss: 0.5930\n",
      "Epoch [5/10], Batch [240], Loss: 0.5858\n",
      "Epoch [5/10], Batch [250], Loss: 0.5765\n",
      "Epoch [5/10], Batch [260], Loss: 0.5840\n",
      "Epoch [5/10], Batch [270], Loss: 0.5827\n",
      "Epoch [5/10], Batch [280], Loss: 0.5864\n",
      "Epoch [5/10], Batch [290], Loss: 0.5962\n",
      "Epoch [5/10], Batch [300], Loss: 0.5790\n",
      "Epoch [5/10], Batch [310], Loss: 0.5828\n",
      "Epoch [5/10], Batch [320], Loss: 0.5800\n",
      "Epoch [5/10], Batch [330], Loss: 0.5851\n",
      "Epoch [5/10], Batch [340], Loss: 0.5771\n",
      "Epoch [5/10], Batch [350], Loss: 0.5778\n",
      "Epoch [5/10], Batch [360], Loss: 0.5890\n",
      "Epoch [5/10], Batch [370], Loss: 0.5821\n",
      "Epoch [5/10], Batch [380], Loss: 0.5825\n",
      "Epoch [5/10], Batch [390], Loss: 0.5810\n",
      "Epoch [5/10], Batch [400], Loss: 0.5805\n",
      "Epoch [5/10], Batch [410], Loss: 0.5877\n",
      "Epoch [5/10], Batch [420], Loss: 0.5827\n",
      "Epoch [5/10], Batch [430], Loss: 0.5854\n",
      "Epoch [5/10], Batch [440], Loss: 0.5812\n",
      "Epoch [5/10], Batch [450], Loss: 0.5797\n",
      "Epoch [5/10], Batch [460], Loss: 0.5828\n",
      "Epoch [5/10], Batch [470], Loss: 0.5771\n",
      "Epoch [5/10], Batch [480], Loss: 0.5815\n",
      "Epoch [5/10], Batch [490], Loss: 0.5819\n",
      "Epoch [5/10], Batch [500], Loss: 0.5770\n",
      "Epoch [5/10], Batch [510], Loss: 0.5853\n",
      "Epoch [5/10], Batch [520], Loss: 0.5795\n",
      "Epoch [5/10], Batch [530], Loss: 0.5862\n",
      "Epoch [5/10], Batch [540], Loss: 0.5862\n",
      "Epoch [5/10], Batch [550], Loss: 0.5860\n",
      "Epoch [5/10], Batch [560], Loss: 0.5905\n",
      "Epoch [5/10], Batch [570], Loss: 0.5841\n",
      "Epoch [5/10], Batch [580], Loss: 0.5847\n",
      "Epoch [5/10], Batch [590], Loss: 0.5808\n",
      "Epoch [5/10], Batch [600], Loss: 0.5826\n",
      "Epoch [5/10], Batch [610], Loss: 0.5849\n",
      "Epoch [5/10], Batch [620], Loss: 0.5838\n",
      "Epoch [5/10], Batch [630], Loss: 0.5827\n",
      "Epoch [5/10], Batch [640], Loss: 0.5837\n",
      "Epoch [5/10], Batch [650], Loss: 0.5820\n",
      "Epoch [6/10], Batch [10], Loss: 0.5825\n",
      "Epoch [6/10], Batch [20], Loss: 0.5775\n",
      "Epoch [6/10], Batch [30], Loss: 0.5799\n",
      "Epoch [6/10], Batch [40], Loss: 0.5872\n",
      "Epoch [6/10], Batch [50], Loss: 0.5845\n",
      "Epoch [6/10], Batch [60], Loss: 0.5912\n",
      "Epoch [6/10], Batch [70], Loss: 0.5837\n",
      "Epoch [6/10], Batch [80], Loss: 0.5836\n",
      "Epoch [6/10], Batch [90], Loss: 0.5815\n",
      "Epoch [6/10], Batch [100], Loss: 0.5823\n",
      "Epoch [6/10], Batch [110], Loss: 0.5814\n",
      "Epoch [6/10], Batch [120], Loss: 0.5823\n",
      "Epoch [6/10], Batch [130], Loss: 0.5804\n",
      "Epoch [6/10], Batch [140], Loss: 0.5774\n",
      "Epoch [6/10], Batch [150], Loss: 0.5848\n",
      "Epoch [6/10], Batch [160], Loss: 0.5747\n",
      "Epoch [6/10], Batch [170], Loss: 0.5781\n",
      "Epoch [6/10], Batch [180], Loss: 0.5872\n",
      "Epoch [6/10], Batch [190], Loss: 0.5845\n",
      "Epoch [6/10], Batch [200], Loss: 0.5867\n",
      "Epoch [6/10], Batch [210], Loss: 0.5777\n",
      "Epoch [6/10], Batch [220], Loss: 0.5825\n",
      "Epoch [6/10], Batch [230], Loss: 0.5878\n",
      "Epoch [6/10], Batch [240], Loss: 0.5808\n",
      "Epoch [6/10], Batch [250], Loss: 0.5761\n",
      "Epoch [6/10], Batch [260], Loss: 0.5840\n",
      "Epoch [6/10], Batch [270], Loss: 0.5746\n",
      "Epoch [6/10], Batch [280], Loss: 0.5824\n",
      "Epoch [6/10], Batch [290], Loss: 0.5808\n",
      "Epoch [6/10], Batch [300], Loss: 0.5732\n",
      "Epoch [6/10], Batch [310], Loss: 0.5823\n",
      "Epoch [6/10], Batch [320], Loss: 0.5848\n",
      "Epoch [6/10], Batch [330], Loss: 0.5766\n",
      "Epoch [6/10], Batch [340], Loss: 0.5794\n",
      "Epoch [6/10], Batch [350], Loss: 0.5819\n",
      "Epoch [6/10], Batch [360], Loss: 0.5844\n",
      "Epoch [6/10], Batch [370], Loss: 0.5792\n",
      "Epoch [6/10], Batch [380], Loss: 0.5820\n",
      "Epoch [6/10], Batch [390], Loss: 0.5841\n",
      "Epoch [6/10], Batch [400], Loss: 0.5855\n",
      "Epoch [6/10], Batch [410], Loss: 0.5744\n",
      "Epoch [6/10], Batch [420], Loss: 0.5883\n",
      "Epoch [6/10], Batch [430], Loss: 0.5823\n",
      "Epoch [6/10], Batch [440], Loss: 0.5801\n",
      "Epoch [6/10], Batch [450], Loss: 0.5899\n",
      "Epoch [6/10], Batch [460], Loss: 0.5783\n",
      "Epoch [6/10], Batch [470], Loss: 0.5848\n",
      "Epoch [6/10], Batch [480], Loss: 0.5842\n",
      "Epoch [6/10], Batch [490], Loss: 0.5737\n",
      "Epoch [6/10], Batch [500], Loss: 0.5794\n",
      "Epoch [6/10], Batch [510], Loss: 0.5905\n",
      "Epoch [6/10], Batch [520], Loss: 0.5729\n",
      "Epoch [6/10], Batch [530], Loss: 0.5774\n",
      "Epoch [6/10], Batch [540], Loss: 0.5816\n",
      "Epoch [6/10], Batch [550], Loss: 0.5851\n",
      "Epoch [6/10], Batch [560], Loss: 0.5837\n",
      "Epoch [6/10], Batch [570], Loss: 0.5929\n",
      "Epoch [6/10], Batch [580], Loss: 0.5784\n",
      "Epoch [6/10], Batch [590], Loss: 0.5811\n",
      "Epoch [6/10], Batch [600], Loss: 0.5790\n",
      "Epoch [6/10], Batch [610], Loss: 0.5824\n",
      "Epoch [6/10], Batch [620], Loss: 0.5809\n",
      "Epoch [6/10], Batch [630], Loss: 0.5802\n",
      "Epoch [6/10], Batch [640], Loss: 0.5820\n",
      "Epoch [6/10], Batch [650], Loss: 0.5831\n",
      "Epoch [7/10], Batch [10], Loss: 0.5855\n",
      "Epoch [7/10], Batch [20], Loss: 0.5811\n",
      "Epoch [7/10], Batch [30], Loss: 0.5824\n",
      "Epoch [7/10], Batch [40], Loss: 0.5806\n",
      "Epoch [7/10], Batch [50], Loss: 0.5825\n",
      "Epoch [7/10], Batch [60], Loss: 0.5789\n",
      "Epoch [7/10], Batch [70], Loss: 0.5816\n",
      "Epoch [7/10], Batch [80], Loss: 0.5776\n",
      "Epoch [7/10], Batch [90], Loss: 0.5792\n",
      "Epoch [7/10], Batch [100], Loss: 0.5782\n",
      "Epoch [7/10], Batch [110], Loss: 0.5774\n",
      "Epoch [7/10], Batch [120], Loss: 0.5780\n",
      "Epoch [7/10], Batch [130], Loss: 0.5897\n",
      "Epoch [7/10], Batch [140], Loss: 0.5844\n",
      "Epoch [7/10], Batch [150], Loss: 0.5792\n",
      "Epoch [7/10], Batch [160], Loss: 0.5757\n",
      "Epoch [7/10], Batch [170], Loss: 0.5864\n",
      "Epoch [7/10], Batch [180], Loss: 0.5855\n",
      "Epoch [7/10], Batch [190], Loss: 0.5764\n",
      "Epoch [7/10], Batch [200], Loss: 0.5869\n",
      "Epoch [7/10], Batch [210], Loss: 0.5807\n",
      "Epoch [7/10], Batch [220], Loss: 0.5827\n",
      "Epoch [7/10], Batch [230], Loss: 0.5778\n",
      "Epoch [7/10], Batch [240], Loss: 0.5720\n",
      "Epoch [7/10], Batch [250], Loss: 0.5807\n",
      "Epoch [7/10], Batch [260], Loss: 0.5808\n",
      "Epoch [7/10], Batch [270], Loss: 0.5864\n",
      "Epoch [7/10], Batch [280], Loss: 0.5794\n",
      "Epoch [7/10], Batch [290], Loss: 0.5875\n",
      "Epoch [7/10], Batch [300], Loss: 0.5746\n",
      "Epoch [7/10], Batch [310], Loss: 0.5869\n",
      "Epoch [7/10], Batch [320], Loss: 0.5877\n",
      "Epoch [7/10], Batch [330], Loss: 0.5870\n",
      "Epoch [7/10], Batch [340], Loss: 0.5779\n",
      "Epoch [7/10], Batch [350], Loss: 0.5739\n",
      "Epoch [7/10], Batch [360], Loss: 0.5817\n",
      "Epoch [7/10], Batch [370], Loss: 0.5835\n",
      "Epoch [7/10], Batch [380], Loss: 0.5758\n",
      "Epoch [7/10], Batch [390], Loss: 0.5760\n",
      "Epoch [7/10], Batch [400], Loss: 0.5842\n",
      "Epoch [7/10], Batch [410], Loss: 0.5776\n",
      "Epoch [7/10], Batch [420], Loss: 0.5814\n",
      "Epoch [7/10], Batch [430], Loss: 0.5884\n",
      "Epoch [7/10], Batch [440], Loss: 0.5812\n",
      "Epoch [7/10], Batch [450], Loss: 0.5787\n",
      "Epoch [7/10], Batch [460], Loss: 0.5811\n",
      "Epoch [7/10], Batch [470], Loss: 0.5725\n",
      "Epoch [7/10], Batch [480], Loss: 0.5796\n",
      "Epoch [7/10], Batch [490], Loss: 0.5812\n",
      "Epoch [7/10], Batch [500], Loss: 0.5844\n",
      "Epoch [7/10], Batch [510], Loss: 0.5766\n",
      "Epoch [7/10], Batch [520], Loss: 0.5782\n",
      "Epoch [7/10], Batch [530], Loss: 0.5817\n",
      "Epoch [7/10], Batch [540], Loss: 0.5875\n",
      "Epoch [7/10], Batch [550], Loss: 0.5801\n",
      "Epoch [7/10], Batch [560], Loss: 0.5806\n",
      "Epoch [7/10], Batch [570], Loss: 0.5744\n",
      "Epoch [7/10], Batch [580], Loss: 0.5805\n",
      "Epoch [7/10], Batch [590], Loss: 0.5844\n",
      "Epoch [7/10], Batch [600], Loss: 0.5860\n",
      "Epoch [7/10], Batch [610], Loss: 0.5900\n",
      "Epoch [7/10], Batch [620], Loss: 0.5784\n",
      "Epoch [7/10], Batch [630], Loss: 0.5888\n",
      "Epoch [7/10], Batch [640], Loss: 0.5859\n",
      "Epoch [7/10], Batch [650], Loss: 0.5772\n",
      "Epoch [8/10], Batch [10], Loss: 0.5827\n",
      "Epoch [8/10], Batch [20], Loss: 0.5811\n",
      "Epoch [8/10], Batch [30], Loss: 0.5753\n",
      "Epoch [8/10], Batch [40], Loss: 0.5775\n",
      "Epoch [8/10], Batch [50], Loss: 0.5872\n",
      "Epoch [8/10], Batch [60], Loss: 0.5815\n",
      "Epoch [8/10], Batch [70], Loss: 0.5834\n",
      "Epoch [8/10], Batch [80], Loss: 0.5783\n",
      "Epoch [8/10], Batch [90], Loss: 0.5800\n",
      "Epoch [8/10], Batch [100], Loss: 0.5817\n",
      "Epoch [8/10], Batch [110], Loss: 0.5842\n",
      "Epoch [8/10], Batch [120], Loss: 0.5816\n",
      "Epoch [8/10], Batch [130], Loss: 0.5747\n",
      "Epoch [8/10], Batch [140], Loss: 0.5905\n",
      "Epoch [8/10], Batch [150], Loss: 0.5846\n",
      "Epoch [8/10], Batch [160], Loss: 0.5860\n",
      "Epoch [8/10], Batch [170], Loss: 0.5826\n",
      "Epoch [8/10], Batch [180], Loss: 0.5832\n",
      "Epoch [8/10], Batch [190], Loss: 0.5766\n",
      "Epoch [8/10], Batch [200], Loss: 0.5909\n",
      "Epoch [8/10], Batch [210], Loss: 0.5824\n",
      "Epoch [8/10], Batch [220], Loss: 0.5809\n",
      "Epoch [8/10], Batch [230], Loss: 0.5719\n",
      "Epoch [8/10], Batch [240], Loss: 0.5826\n",
      "Epoch [8/10], Batch [250], Loss: 0.5847\n",
      "Epoch [8/10], Batch [260], Loss: 0.5809\n",
      "Epoch [8/10], Batch [270], Loss: 0.5838\n",
      "Epoch [8/10], Batch [280], Loss: 0.5855\n",
      "Epoch [8/10], Batch [290], Loss: 0.5813\n",
      "Epoch [8/10], Batch [300], Loss: 0.5783\n",
      "Epoch [8/10], Batch [310], Loss: 0.5822\n",
      "Epoch [8/10], Batch [320], Loss: 0.5886\n",
      "Epoch [8/10], Batch [330], Loss: 0.5833\n",
      "Epoch [8/10], Batch [340], Loss: 0.5812\n",
      "Epoch [8/10], Batch [350], Loss: 0.5799\n",
      "Epoch [8/10], Batch [360], Loss: 0.5782\n",
      "Epoch [8/10], Batch [370], Loss: 0.5812\n",
      "Epoch [8/10], Batch [380], Loss: 0.5776\n",
      "Epoch [8/10], Batch [390], Loss: 0.5820\n",
      "Epoch [8/10], Batch [400], Loss: 0.5830\n",
      "Epoch [8/10], Batch [410], Loss: 0.5785\n",
      "Epoch [8/10], Batch [420], Loss: 0.5851\n",
      "Epoch [8/10], Batch [430], Loss: 0.5784\n",
      "Epoch [8/10], Batch [440], Loss: 0.5780\n",
      "Epoch [8/10], Batch [450], Loss: 0.5720\n",
      "Epoch [8/10], Batch [460], Loss: 0.5719\n",
      "Epoch [8/10], Batch [470], Loss: 0.5809\n",
      "Epoch [8/10], Batch [480], Loss: 0.5811\n",
      "Epoch [8/10], Batch [490], Loss: 0.5786\n",
      "Epoch [8/10], Batch [500], Loss: 0.5832\n",
      "Epoch [8/10], Batch [510], Loss: 0.5846\n",
      "Epoch [8/10], Batch [520], Loss: 0.5724\n",
      "Epoch [8/10], Batch [530], Loss: 0.5770\n",
      "Epoch [8/10], Batch [540], Loss: 0.5811\n",
      "Epoch [8/10], Batch [550], Loss: 0.5835\n",
      "Epoch [8/10], Batch [560], Loss: 0.5736\n",
      "Epoch [8/10], Batch [570], Loss: 0.5758\n",
      "Epoch [8/10], Batch [580], Loss: 0.5842\n",
      "Epoch [8/10], Batch [590], Loss: 0.5809\n",
      "Epoch [8/10], Batch [600], Loss: 0.5732\n",
      "Epoch [8/10], Batch [610], Loss: 0.5792\n",
      "Epoch [8/10], Batch [620], Loss: 0.5805\n",
      "Epoch [8/10], Batch [630], Loss: 0.5723\n",
      "Epoch [8/10], Batch [640], Loss: 0.5801\n",
      "Epoch [8/10], Batch [650], Loss: 0.5814\n",
      "Epoch [9/10], Batch [10], Loss: 0.5837\n",
      "Epoch [9/10], Batch [20], Loss: 0.5850\n",
      "Epoch [9/10], Batch [30], Loss: 0.5830\n",
      "Epoch [9/10], Batch [40], Loss: 0.5830\n",
      "Epoch [9/10], Batch [50], Loss: 0.5849\n",
      "Epoch [9/10], Batch [60], Loss: 0.5824\n",
      "Epoch [9/10], Batch [70], Loss: 0.5781\n",
      "Epoch [9/10], Batch [80], Loss: 0.5801\n",
      "Epoch [9/10], Batch [90], Loss: 0.5839\n",
      "Epoch [9/10], Batch [100], Loss: 0.5777\n",
      "Epoch [9/10], Batch [110], Loss: 0.5768\n",
      "Epoch [9/10], Batch [120], Loss: 0.5747\n",
      "Epoch [9/10], Batch [130], Loss: 0.5792\n",
      "Epoch [9/10], Batch [140], Loss: 0.5818\n",
      "Epoch [9/10], Batch [150], Loss: 0.5866\n",
      "Epoch [9/10], Batch [160], Loss: 0.5754\n",
      "Epoch [9/10], Batch [170], Loss: 0.5756\n",
      "Epoch [9/10], Batch [180], Loss: 0.5783\n",
      "Epoch [9/10], Batch [190], Loss: 0.5846\n",
      "Epoch [9/10], Batch [200], Loss: 0.5872\n",
      "Epoch [9/10], Batch [210], Loss: 0.5810\n",
      "Epoch [9/10], Batch [220], Loss: 0.5865\n",
      "Epoch [9/10], Batch [230], Loss: 0.5778\n",
      "Epoch [9/10], Batch [240], Loss: 0.5767\n",
      "Epoch [9/10], Batch [250], Loss: 0.5775\n",
      "Epoch [9/10], Batch [260], Loss: 0.5765\n",
      "Epoch [9/10], Batch [270], Loss: 0.5726\n",
      "Epoch [9/10], Batch [280], Loss: 0.5805\n",
      "Epoch [9/10], Batch [290], Loss: 0.5787\n",
      "Epoch [9/10], Batch [300], Loss: 0.5827\n",
      "Epoch [9/10], Batch [310], Loss: 0.5774\n",
      "Epoch [9/10], Batch [320], Loss: 0.5780\n",
      "Epoch [9/10], Batch [330], Loss: 0.5766\n",
      "Epoch [9/10], Batch [340], Loss: 0.5812\n",
      "Epoch [9/10], Batch [350], Loss: 0.5829\n",
      "Epoch [9/10], Batch [360], Loss: 0.5780\n",
      "Epoch [9/10], Batch [370], Loss: 0.5844\n",
      "Epoch [9/10], Batch [380], Loss: 0.5804\n",
      "Epoch [9/10], Batch [390], Loss: 0.5792\n",
      "Epoch [9/10], Batch [400], Loss: 0.5771\n",
      "Epoch [9/10], Batch [410], Loss: 0.5815\n",
      "Epoch [9/10], Batch [420], Loss: 0.5783\n",
      "Epoch [9/10], Batch [430], Loss: 0.5787\n",
      "Epoch [9/10], Batch [440], Loss: 0.5908\n",
      "Epoch [9/10], Batch [450], Loss: 0.5768\n",
      "Epoch [9/10], Batch [460], Loss: 0.5783\n",
      "Epoch [9/10], Batch [470], Loss: 0.5871\n",
      "Epoch [9/10], Batch [480], Loss: 0.5780\n",
      "Epoch [9/10], Batch [490], Loss: 0.5746\n",
      "Epoch [9/10], Batch [500], Loss: 0.5825\n",
      "Epoch [9/10], Batch [510], Loss: 0.5786\n",
      "Epoch [9/10], Batch [520], Loss: 0.5759\n",
      "Epoch [9/10], Batch [530], Loss: 0.5727\n",
      "Epoch [9/10], Batch [540], Loss: 0.5862\n",
      "Epoch [9/10], Batch [550], Loss: 0.5769\n",
      "Epoch [9/10], Batch [560], Loss: 0.5812\n",
      "Epoch [9/10], Batch [570], Loss: 0.5804\n",
      "Epoch [9/10], Batch [580], Loss: 0.5832\n",
      "Epoch [9/10], Batch [590], Loss: 0.5779\n",
      "Epoch [9/10], Batch [600], Loss: 0.5878\n",
      "Epoch [9/10], Batch [610], Loss: 0.5829\n",
      "Epoch [9/10], Batch [620], Loss: 0.5853\n",
      "Epoch [9/10], Batch [630], Loss: 0.5797\n",
      "Epoch [9/10], Batch [640], Loss: 0.5755\n",
      "Epoch [9/10], Batch [650], Loss: 0.5824\n",
      "Epoch [10/10], Batch [10], Loss: 0.5853\n",
      "Epoch [10/10], Batch [20], Loss: 0.5758\n",
      "Epoch [10/10], Batch [30], Loss: 0.5738\n",
      "Epoch [10/10], Batch [40], Loss: 0.5769\n",
      "Epoch [10/10], Batch [50], Loss: 0.5816\n",
      "Epoch [10/10], Batch [60], Loss: 0.5784\n",
      "Epoch [10/10], Batch [70], Loss: 0.5769\n",
      "Epoch [10/10], Batch [80], Loss: 0.5828\n",
      "Epoch [10/10], Batch [90], Loss: 0.5828\n",
      "Epoch [10/10], Batch [100], Loss: 0.5771\n",
      "Epoch [10/10], Batch [110], Loss: 0.5819\n",
      "Epoch [10/10], Batch [120], Loss: 0.5809\n",
      "Epoch [10/10], Batch [130], Loss: 0.5805\n",
      "Epoch [10/10], Batch [140], Loss: 0.5893\n",
      "Epoch [10/10], Batch [150], Loss: 0.5832\n",
      "Epoch [10/10], Batch [160], Loss: 0.5841\n",
      "Epoch [10/10], Batch [170], Loss: 0.5779\n",
      "Epoch [10/10], Batch [180], Loss: 0.5824\n",
      "Epoch [10/10], Batch [190], Loss: 0.5790\n",
      "Epoch [10/10], Batch [200], Loss: 0.5809\n",
      "Epoch [10/10], Batch [210], Loss: 0.5816\n",
      "Epoch [10/10], Batch [220], Loss: 0.5827\n",
      "Epoch [10/10], Batch [230], Loss: 0.5865\n",
      "Epoch [10/10], Batch [240], Loss: 0.5850\n",
      "Epoch [10/10], Batch [250], Loss: 0.5832\n",
      "Epoch [10/10], Batch [260], Loss: 0.5815\n",
      "Epoch [10/10], Batch [270], Loss: 0.5817\n",
      "Epoch [10/10], Batch [280], Loss: 0.5697\n",
      "Epoch [10/10], Batch [290], Loss: 0.5833\n",
      "Epoch [10/10], Batch [300], Loss: 0.5818\n",
      "Epoch [10/10], Batch [310], Loss: 0.5782\n",
      "Epoch [10/10], Batch [320], Loss: 0.5746\n",
      "Epoch [10/10], Batch [330], Loss: 0.5728\n",
      "Epoch [10/10], Batch [340], Loss: 0.5732\n",
      "Epoch [10/10], Batch [350], Loss: 0.5794\n",
      "Epoch [10/10], Batch [360], Loss: 0.5745\n",
      "Epoch [10/10], Batch [370], Loss: 0.5776\n",
      "Epoch [10/10], Batch [380], Loss: 0.5740\n",
      "Epoch [10/10], Batch [390], Loss: 0.5815\n",
      "Epoch [10/10], Batch [400], Loss: 0.5800\n",
      "Epoch [10/10], Batch [410], Loss: 0.5780\n",
      "Epoch [10/10], Batch [420], Loss: 0.5815\n",
      "Epoch [10/10], Batch [430], Loss: 0.5817\n",
      "Epoch [10/10], Batch [440], Loss: 0.5746\n",
      "Epoch [10/10], Batch [450], Loss: 0.5772\n",
      "Epoch [10/10], Batch [460], Loss: 0.5885\n",
      "Epoch [10/10], Batch [470], Loss: 0.5738\n",
      "Epoch [10/10], Batch [480], Loss: 0.5779\n",
      "Epoch [10/10], Batch [490], Loss: 0.5825\n",
      "Epoch [10/10], Batch [500], Loss: 0.5819\n",
      "Epoch [10/10], Batch [510], Loss: 0.5789\n",
      "Epoch [10/10], Batch [520], Loss: 0.5823\n",
      "Epoch [10/10], Batch [530], Loss: 0.5828\n",
      "Epoch [10/10], Batch [540], Loss: 0.5816\n",
      "Epoch [10/10], Batch [550], Loss: 0.5809\n",
      "Epoch [10/10], Batch [560], Loss: 0.5824\n",
      "Epoch [10/10], Batch [570], Loss: 0.5784\n",
      "Epoch [10/10], Batch [580], Loss: 0.5765\n",
      "Epoch [10/10], Batch [590], Loss: 0.5801\n",
      "Epoch [10/10], Batch [600], Loss: 0.5853\n",
      "Epoch [10/10], Batch [610], Loss: 0.5824\n",
      "Epoch [10/10], Batch [620], Loss: 0.5748\n",
      "Epoch [10/10], Batch [630], Loss: 0.5739\n",
      "Epoch [10/10], Batch [640], Loss: 0.5846\n",
      "Epoch [10/10], Batch [650], Loss: 0.5818\n"
     ]
    }
   ],
   "source": [
    "def train_model_with_checks(model, trainloader, criterion, optimizer, num_epochs=10, clip_value=1.0):\n",
    "    \"\"\"\n",
    "    Training loop with additional checks to prevent NaN losses\n",
    "    \"\"\"\n",
    "    model.to(device) \n",
    "    model.train()\n",
    "    \n",
    "    # Track losses for monitoring\n",
    "    all_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            signal_features, labels = data\n",
    "            signal_features = signal_features.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                # Forward pass\n",
    "                outputs = model(signal_features)\n",
    "                loss = criterion(outputs, labels.view(-1, 1))\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track loss\n",
    "                all_losses.append(loss.item())\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                if i % 10 == 9:\n",
    "                    avg_loss = running_loss / 10\n",
    "                    print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}], Loss: {avg_loss:.4f}')\n",
    "                    running_loss = 0.0\n",
    "                    \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Runtime Error in batch {i}:\", e)\n",
    "                continue\n",
    "    \n",
    "    return all_losses\n",
    "\n",
    "# Additional helper functions for model debugging\n",
    "def check_model_weights(model):\n",
    "    \"\"\"Check if model weights are properly initialized\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"\\nLayer: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"Mean: {param.mean().item():.4f}\")\n",
    "        print(f\"Std: {param.std().item():.4f}\")\n",
    "        print(f\"Min: {param.min().item():.4f}\")\n",
    "        print(f\"Max: {param.max().item():.4f}\")\n",
    "\n",
    "def initialize_weights(model):\n",
    "    \"\"\"Initialize model weights properly\"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (torch.nn.Linear, torch.nn.Conv1d)):\n",
    "            torch.nn.init.kaiming_normal_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "# print(\"Checking model weights before training:\")\n",
    "# check_model_weights(model)\n",
    "\n",
    "# # Initialize weights properly\n",
    "# initialize_weights(model)\n",
    "\n",
    "# print(\"\\nChecking model weights after initialization:\")\n",
    "# check_model_weights(model)\n",
    "\n",
    "# Modified optimizer with gradient clipping\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "losses = train_model_with_checks(model, trainloader, criterion, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5807, ROC-AUC: 0.9303, PR-AUC: 0.9249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5806522011756897,\n",
       " np.float64(0.9303084699078698),\n",
       " np.float64(0.9249033300115246))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "# Function to evaluate on the test set\n",
    "def evaluate_model(model, testloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            signal_features, labels = data\n",
    "            \n",
    "            # Move data to device\n",
    "            signal_features = signal_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            read_level_probs = model(signal_features)\n",
    "            site_level_probs = model.noisy_or_pooling(read_level_probs).squeeze()  # Shape: (batch_size,)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(site_level_probs, labels.float())\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and labels for ROC and PR AUC\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_predictions.append(site_level_probs.cpu())\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "    # Compute ROC-AUC\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    # Compute PR-AUC\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, all_predictions)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    # Average loss\n",
    "    avg_loss = total_loss / len(testloader)\n",
    "    \n",
    "    print(f'Test Loss: {avg_loss:.4f}, ROC-AUC: {roc_auc:.4f}, PR-AUC: {pr_auc:.4f}')\n",
    "    \n",
    "    return avg_loss, roc_auc, pr_auc\n",
    "\n",
    "evaluate_model(model, testloader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
